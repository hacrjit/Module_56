{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "572f756e-dc5b-4644-8021-61f787c14b84",
   "metadata": {},
   "source": [
    "### <b>Question No. 1</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44a084d-59db-409f-a79b-57eaedcebd04",
   "metadata": {},
   "source": [
    "Min-Max scaling is a technique used in data preprocessing to scale and normalize numerical features in a dataset to a specific range, typically between 0 and 1. It is calculated using the formula:\n",
    "\n",
    "X_scaled = (X - X_min) / (X_max - X_min)\n",
    "\n",
    "where:\n",
    "- X is the original value of the feature.\n",
    "- X_min is the minimum value of the feature in the dataset.\n",
    "- X_max is the maximum value of the feature in the dataset.\n",
    "- X_scaled is the scaled value of the feature.\n",
    "\n",
    "Min-Max scaling is beneficial because it ensures that all features are on a similar scale, which can improve the performance of machine learning models. It prevents features with larger values from dominating the model.\n",
    "\n",
    "Example:\n",
    "Suppose we have a dataset with a feature 'Income' that ranges from 20,000 to 100,000. We want to scale this feature to a range of [0, 1] using Min-Max scaling. The minimum income in the dataset is 20,000, and the maximum income is 100,000.\n",
    "\n",
    "For a given income, say 50,000, the scaled value would be:\n",
    "\n",
    "X_scaled = (50,000 - 20,000) / (100,000 - 20,000)\n",
    "         = 30,000 / 80,000\n",
    "         = 0.375\n",
    "\n",
    "So, the scaled value of income 50,000 would be 0.375 after Min-Max scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b0ac7d-9080-45dc-b170-dd1e8c2e18a2",
   "metadata": {},
   "source": [
    "### <b>Question No. 2</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77187b6e-73ba-46b0-88ee-900fb6a4b777",
   "metadata": {},
   "source": [
    "The Unit Vector technique in feature scaling, also known as normalization, is a method used to scale the values of a feature to have a unit norm (length). It is calculated by dividing each data point by the Euclidean norm of the feature vector. The formula for calculating the unit vector is:\n",
    "\n",
    "Unit Vector = X / ||X||\n",
    "\n",
    "where:\n",
    "- X is the original value of the feature.\n",
    "- ||X|| is the Euclidean norm of the feature vector.\n",
    "\n",
    "The Euclidean norm of a vector X = [x₁, x₂, ..., xₙ] is calculated as:\n",
    "\n",
    "||X|| = sqrt(x₁² + x₂² + ... + xₙ²)\n",
    "\n",
    "The Unit Vector technique ensures that each feature vector has a length of 1, which can be useful in algorithms that rely on the direction of the vectors rather than their magnitude, such as in text classification or clustering algorithms.\n",
    "\n",
    "Example:\n",
    "Suppose we have a dataset with a feature 'Vector' represented as [3, 4]. To calculate the unit vector for this feature, we first calculate the Euclidean norm:\n",
    "\n",
    "||X|| = sqrt(3² + 4²) = sqrt(9 + 16) = sqrt(25) = 5\n",
    "\n",
    "Then, we calculate the unit vector:\n",
    "\n",
    "Unit Vector = [3, 4] / 5 = [0.6, 0.8]\n",
    "\n",
    "So, the unit vector for the feature [3, 4] is [0.6, 0.8].\n",
    "\n",
    "Difference from Min-Max Scaling:\n",
    "- Min-Max scaling scales the values of a feature to a specific range (e.g., [0, 1]), whereas the Unit Vector technique scales the feature values to have a unit norm (length).\n",
    "- Min-Max scaling is useful for algorithms that require features to be on a similar scale, while the Unit Vector technique is useful for algorithms that focus on the direction of the feature vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cf2561-5cd2-4782-8f3a-3ada37a31e9b",
   "metadata": {},
   "source": [
    "### <b>Question No. 3</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f30610-9fa5-4a19-a2ae-5e4966767038",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis) is a technique used for dimensionality reduction in which the original features of a dataset are transformed into a new set of orthogonal (uncorrelated) features called principal components. These principal components are ordered in such a way that the first few components retain most of the variance in the data. This allows for a lower-dimensional representation of the data while preserving as much of the original information as possible.\n",
    "\n",
    "PCA works by finding the directions (principal components) in which the data varies the most. The first principal component is the direction along which the data varies the most, the second principal component is the direction orthogonal to the first component along which the data varies the most, and so on.\n",
    "\n",
    "PCA is used in dimensionality reduction to reduce the number of features in a dataset, which can:\n",
    "1. Simplify the dataset and make it easier to visualize.\n",
    "2. Speed up computation by reducing the number of features.\n",
    "3. Remove noise and redundant information.\n",
    "\n",
    "Example:\n",
    "Suppose we have a dataset with two features, 'Height' and 'Weight', and we want to reduce it to one dimension using PCA. We start by standardizing the features (subtracting the mean and dividing by the standard deviation). Then, we calculate the covariance matrix of the standardized features and find its eigenvectors and eigenvalues. The eigenvector corresponding to the largest eigenvalue is the first principal component. We project the data onto this principal component to obtain the reduced-dimensional representation of the data.\n",
    "\n",
    "Here's a simplified example using synthetic data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72f9634d-132f-4aa2-9e93-5e9e0377d148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (100, 2)\n",
      "Reduced shape: (100, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create synthetic data\n",
    "np.random.seed(0)\n",
    "data = np.random.randn(100, 2)  # 100 samples, 2 features\n",
    "\n",
    "# Fit PCA and transform data\n",
    "pca = PCA(n_components=1)  # Reduce to 1 dimension\n",
    "data_reduced = pca.fit_transform(data)\n",
    "\n",
    "print(\"Original shape:\", data.shape)\n",
    "print(\"Reduced shape:\", data_reduced.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5592ceaf-81ae-43d6-887d-e8fffd4d0213",
   "metadata": {},
   "source": [
    "In this example, the original dataset has 2 features, but after applying PCA with `n_components=1`, the dataset is reduced to 1 dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2cb79-9671-4ce6-aa0b-46a95ee42458",
   "metadata": {},
   "source": [
    "### <b>Question No. 4</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4027a65b-3788-4382-9352-6cd25ebb4adc",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis) is a technique commonly used for feature extraction. Feature extraction is the process of transforming raw data into a set of new features (or variables) that are more informative and can help improve the performance of machine learning algorithms. PCA is one such technique that can be used for feature extraction by transforming the original features into a smaller set of new features called principal components.\n",
    "\n",
    "The relationship between PCA and feature extraction lies in the fact that PCA extracts the most important information (variance) from the original features and represents it in a new, lower-dimensional space. These new features (principal components) are a linear combination of the original features and are orthogonal to each other, meaning they are uncorrelated.\n",
    "\n",
    "PCA can be used for feature extraction in several ways:\n",
    "1. **Dimensionality Reduction:** PCA can be used to reduce the number of features in a dataset by selecting only the most important principal components that capture the majority of the variance in the data. This can help reduce overfitting and improve the performance of machine learning algorithms.\n",
    "\n",
    "2. **Noise Reduction:** PCA can also be used to reduce noise in the data by focusing on the principal components that capture the signal and ignoring those that represent noise.\n",
    "\n",
    "3. **Visualization:** PCA can be used to visualize high-dimensional data in a lower-dimensional space (e.g., 2D or 3D) by plotting the data using the first few principal components.\n",
    "\n",
    "Example:\n",
    "Suppose we have a dataset with 100 samples and 10 features. We can use PCA to extract the most important features (principal components) from this dataset. Here's a simplified example using synthetic data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d795f8b-5881-47c4-a739-d23b2435f893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (100, 10)\n",
      "Transformed shape: (100, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create synthetic data\n",
    "np.random.seed(0)\n",
    "data = np.random.randn(100, 10)  # 100 samples, 10 features\n",
    "\n",
    "# Fit PCA and transform data\n",
    "pca = PCA(n_components=3)  # Extract 3 principal components\n",
    "data_transformed = pca.fit_transform(data)\n",
    "\n",
    "print(\"Original shape:\", data.shape)\n",
    "print(\"Transformed shape:\", data_transformed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0acf8e2-57de-43cb-8177-ffb4e26b6f44",
   "metadata": {},
   "source": [
    "In this example, we use PCA to extract 3 principal components from the original 10 features. The transformed data now has only 3 features (principal components), which capture the most important information from the original dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1340d6a1-8935-456b-afda-416cf38776a8",
   "metadata": {},
   "source": [
    "### <b>Question No. 5</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d38885e-c5e2-40fd-b14e-5d44b6d76158",
   "metadata": {},
   "source": [
    "To preprocess the data for building a recommendation system for a food delivery service using Min-Max scaling, follow these steps:\n",
    "\n",
    "1. **Understand the Data:** Start by understanding the dataset, including the range and distribution of each feature (price, rating, delivery time).\n",
    "\n",
    "2. **Min-Max Scaling:** Min-Max scaling is used to scale the values of each feature to a specific range, typically [0, 1]. This is done to ensure that all features are on a similar scale and no single feature dominates the others.\n",
    "\n",
    "3. **Calculate Min-Max Scaling:** For each feature, calculate the minimum value (X_min) and maximum value (X_max) in the dataset.\n",
    "\n",
    "4. **Apply Min-Max Scaling:** For each data point in the dataset, apply the Min-Max scaling formula:\n",
    "\n",
    "   X_scaled = (X - X_min) / (X_max - X_min)\n",
    "\n",
    "   where:\n",
    "   - X is the original value of the feature.\n",
    "   - X_min is the minimum value of the feature in the dataset.\n",
    "   - X_max is the maximum value of the feature in the dataset.\n",
    "   - X_scaled is the scaled value of the feature.\n",
    "\n",
    "5. **Apply to all Features:** Repeat the scaling process for all features in the dataset (price, rating, delivery time).\n",
    "\n",
    "6. **Normalization Range:** After scaling, all features will be in the range [0, 1]. This ensures that features with larger values (e.g., price) do not dominate features with smaller values (e.g., rating).\n",
    "\n",
    "7. **Data Preprocessing:** Use the scaled features as input for building the recommendation system. The scaled features will provide a more balanced representation of the data and improve the performance of the recommendation system.\n",
    "\n",
    "In summary, Min-Max scaling is used to preprocess the data for a recommendation system by scaling the features to a specific range, ensuring that all features are on a similar scale and improving the performance of the recommendation system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f03547-625b-4837-bd90-3468ff87bc5e",
   "metadata": {},
   "source": [
    "### <b>Question No. 6</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d00c20b-78ae-4a91-8fcd-ca55be8b769f",
   "metadata": {},
   "source": [
    "To use PCA to reduce the dimensionality of a dataset containing features for predicting stock prices, you can follow these steps:\n",
    "\n",
    "1. **Load the Dataset:** Load the dataset containing features such as company financial data (e.g., revenue, profit margin, debt-to-equity ratio) and market trends (e.g., stock price volatility, industry performance).\n",
    "\n",
    "2. **Preprocess the Data:** Standardize the dataset by subtracting the mean and dividing by the standard deviation for each feature. This step ensures that all features are on a similar scale, which is important for PCA.\n",
    "\n",
    "3. **Apply PCA:** Use PCA to reduce the dimensionality of the dataset. Choose the number of principal components based on the explained variance ratio. For example, you may decide to keep enough components to explain 95% of the variance in the data.\n",
    "\n",
    "4. **Fit PCA:** Fit the PCA model to the standardized dataset and transform the dataset into the reduced-dimensional space.\n",
    "\n",
    "5. **Use Reduced Dataset:** Use the reduced dataset as input for building the model to predict stock prices. The reduced dataset will have fewer features (principal components) while capturing most of the variance in the original dataset.\n",
    "\n",
    "Here's a simplified example using Python and the `pandas` and `sklearn` libraries:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('stock_data.csv')\n",
    "\n",
    "# Separate features and target variable\n",
    "X = data.drop('stock_price', axis=1)  # Features\n",
    "y = data['stock_price']  # Target variable\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=0.95)  # Keep components that explain 95% of variance\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Check the number of components\n",
    "n_components = pca.n_components_\n",
    "\n",
    "# Use X_pca for modeling\n",
    "```\n",
    "\n",
    "In this example, `stock_data.csv` is the dataset containing features and the target variable (stock price). We standardize the features, apply PCA to reduce dimensionality while retaining 95% of the variance, and then use the reduced dataset `X_pca` for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da22b532-2321-4217-a7ec-d79776d83110",
   "metadata": {},
   "source": [
    "### <b>Question No. 7</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c697808-839b-4a2a-9e9e-984b6c7d4a24",
   "metadata": {},
   "source": [
    "Here's the Min-Max scaling formula and the scaled values for the dataset \\([1, 5, 10, 15, 20]\\) in the range of -1 to 1 in a simple and normal font:\n",
    "\n",
    "Min-Max scaling formula:\n",
    "\n",
    "X_scaled = (X - X_min) / (X_max - X_min) * (max_new - min_new) + min_new\n",
    "\n",
    "For this dataset, the minimum value X_min is 1, the maximum value X_max is 20, and the new range is -1 to 1.\n",
    "\n",
    "1. For X = 1:\n",
    "   X_scaled = (1 - 1) / (20 - 1) * (1 - (-1)) + (-1) =  -1\n",
    "\n",
    "2. For X = 5:\n",
    "   X_scaled = (5 - 1) / (20 - 1) * (1 - (-1)) + (-1) ≈  -0.6\n",
    "\n",
    "3. For X = 10:\n",
    "   X_scaled = (10 - 1) / (20 - 1) * (1 - (-1)) + (-1) = 0\n",
    "\n",
    "4. For X = 15:\n",
    "   X_scaled = (15 - 1) / (20 - 1) * (1 - (-1)) + (-1) ≈ 0.6\n",
    "\n",
    "5. For X = 20:\n",
    "   X_scaled = (20 - 1) / (20 - 1) * (1 - (-1)) + (-1) = 1\n",
    "\n",
    "Therefore, the Min-Max scaled values for the dataset \\([1, 5, 10, 15, 20]\\) in the range of -1 to 1 are approximately \\([-1, -0.6, 0, 0.6, 1]\\)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ca6cc4-03d7-45f0-b949-ce98687e39d9",
   "metadata": {},
   "source": [
    "### <b>Question No. 8</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddcca17-5b37-4c03-aedf-cbee40bb4733",
   "metadata": {},
   "source": [
    "To perform feature extraction using Principal Component Analysis (PCA) on the dataset containing the features [height, weight, age, gender, blood pressure], we would first need to preprocess the data by standardizing it (subtracting the mean and dividing by the standard deviation) to ensure that each feature contributes equally to the PCA.\n",
    "\n",
    "After standardizing the data, we would compute the covariance matrix and then calculate the eigenvectors and eigenvalues. The eigenvectors represent the directions of the new feature space, and the eigenvalues represent the magnitude of the variance in those directions.\n",
    "\n",
    "The number of principal components (PCs) to retain depends on the explained variance ratio and the desired level of information retention. The explained variance ratio tells us the proportion of the dataset's variance that lies along each principal component. A common approach is to choose the number of principal components that explain a significant portion of the variance in the data, such as 95% or 99%.\n",
    "\n",
    "For example, if the first three principal components explain 95% of the variance, you might choose to retain these three components.\n",
    "\n",
    "In practice, you would typically visualize the explained variance ratio and choose the number of components that provide a good balance between retaining enough information and reducing the dimensionality of the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
